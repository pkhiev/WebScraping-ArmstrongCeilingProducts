# -*- coding: utf-8 -*-
"""ArmstrongScraper2.0

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tHx_hy-NG4sEOGpR5g9y-DHMGtlXa3BO
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# sudo apt -y update
# sudo apt install -y wget curl unzip
# wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb
# dpkg -i libu2f-udev_1.1.4-1_all.deb
# wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
# dpkg -i google-chrome-stable_current_amd64.deb
# 
# wget -N https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/118.0.5993.70/linux64/chromedriver-linux64.zip -P /tmp/
# unzip -o /tmp/chromedriver-linux64.zip -d /tmp/
# chmod +x /tmp/chromedriver-linux64/chromedriver
# mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
# pip install selenium chromedriver_autoinstaller

import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

from selenium import webdriver
import chromedriver_autoinstaller

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless') # this is must
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chromedriver_autoinstaller.install()

driver = webdriver.Chrome(options=chrome_options)

from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
import json
import csv

TRACKED_SPECS = ['Materials', 'Sound Absorption (NRC)', 'Sound Blocking (CAC)', 'Light Reflectance', 'Sag/Humidity Resistance', 'Fire Performance']

#!apt --fix-broken install
#!apt install python3.10-venv

#!python -m venv env

#For URL to an individual product page
def scrape_product_page(url):

  products = []
  product = dict.fromkeys(TRACKED_SPECS)
  driver.get(url)
  time.sleep(3)
  product_soup = BeautifulSoup(driver.page_source)

  try:
    product_name = product_soup.select_one('.hero-headline').get_text(strip=True)
  except:
    return
  product['Product'] = product_name

  specs = product_soup.select('.attribute')
  for spec in specs:
    spec_category = spec.div.contents[0].strip()
    try:
      value = spec.select_one('.text-right').get_text(strip=True)
    except:
      value = None
    #print(category+': ' + value)
    if spec_category in TRACKED_SPECS:
      product[spec_category] = value
  products.append(product)
  return products

#Use for url containing a family of products.
#Scrapes product data for an entire product line
def scrape_family_of_products(url):
  products = []
  driver.get(url)
  time.sleep(3)
  soup = BeautifulSoup(driver.page_source)
  soup.select('.browse-result-item')
  product_items = soup.select('.browse-result-item')

  for product_item in product_items:
    product = scrape_product_of_family(product_item)
    products.append(product)

  return products

#Scrapes individual product data for each product in a family of products
def scrape_product_of_family(product):
  product_name = product.select_one('h3').get_text()
  specs = product.select('tr')
  product = dict.fromkeys(TRACKED_SPECS)
  product['Product'] = product_name
  for spec in specs:
    spec_category = spec.select_one('th').get_text()
    spec_value = spec.select_one('td').get_text()
    if spec_category in TRACKED_SPECS:
      product[spec_category] = spec_value

  return product


# Links for testing functions

#scrape_product_page('https://www.armstrongceilings.com/commercial/en/commercial-ceilings-walls/ceilings/acoustibuilt-ceiling-family.html')
#scrape_family_of_products('https://www.armstrongceilings.com/commercial/en/commercial-ceilings-walls/ceilings/calla-ceiling-family.html')

"""COLLECTING URLS FOR PRODUCT PAGES"""

#Webcrawler to collect URLS to each product page
domain = 'https://www.armstrongceilings.com'

driver.get('https://www.armstrongceilings.com//commercial/en/commercial-ceilings-walls/ceilings.html')
time.sleep(2)

#find show more button and click it until it is no longer clickable to load all items onto the page
show_more_button = driver.find_element(By.CSS_SELECTOR, "#article1 > div > section.section.show-more > div > div > button")

while show_more_button.is_displayed() and show_more_button.is_enabled():
  show_more_button.click()
  time.sleep(2)

#Create a list of product URLS
urls = []
soup = BeautifulSoup(driver.page_source)
link_elements = soup.select("a[href]")

for link_element in link_elements:
  url = link_element['href']
  if "en/commercial-ceilings-walls/" and "family" in url: #or "commercial-ceilings-walls/ceilings"
    if url not in urls:
      urls.append(url)

"""Web Crawling through product pages + Scraping"""

#Web Crawling through product pages + Scraping product Data
product_list = []

for url in urls:
  driver.get(domain+url)
  time.sleep(2)
  current_url = driver.current_url
  if 'family' in current_url:
    products = scrape_family_of_products(current_url)
    if(products):
      product_list.extend(products)
  else:
    products = scrape_product_page(current_url)
    if(products):
      product_list.extend(products)

"""Saving data to JSON/CSV"""

#Store Data

filename= 'ArmstrongProductList'

#Save data to json
#with open(filename+'.json', 'w') as fout:
  #json.dump(product_list, fout)


#save file as CSV
keys = product_list[0].keys()

with open(filename+'.csv', 'w', newline='') as output_file:
  dict_writer = csv.DictWriter(output_file, keys)
  dict_writer.writeheader()
  dict_writer.writerows(product_list)